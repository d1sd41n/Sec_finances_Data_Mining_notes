{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping the SEC Query Page\n",
    "\n",
    "Here we will explore a different part of the EDGAR database. The company search page allows us to make a specific query for a single company and their filings, and this page will then return all the documents related to that company. From here, we can filter all of their documents to the ones that meet our criteria.\n",
    "\n",
    "This includes being able to filter by a specific date or even a particular type of form. Once, we've filtered the results we can go directly to the document or if we want we can go to the filing folder containing that document. One thing to keep in mind is the scope of your search. If you search for a company name, you can get back more than one company back.\n",
    "\n",
    "This usually doesn't present a problem, but it does mean you may have to look through multiple companies to find the documents you want. It might make more sense to search by the CIK number, to get to the company you want.\n",
    "\n",
    "Link to the company search page: https://www.sec.gov/edgar/searchedgar/companysearch.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section One: Define the Parameters of the Search\n",
    "To create a search we need to \"build\" a URL that takes us to a valid results query, this requires taking our base endpoint and attaching on different parameters to help narrow down our search. I'll do my best to explain how each of these parameters works, but unfortunately, there is no formal documentation on this.\n",
    "\n",
    "Endpoint The endpoint for our EDGAR query is https://www.sec.gov/cgi-bin/browse-edgar if you go to this link without any additional parameters it will be an invalid request.\n",
    "\n",
    "--------------------------------------------------------------------\n",
    "### Parameters:\n",
    "\n",
    "- **action:** (required) By default should be set to getcompany.\n",
    "\n",
    "- **CIK**: (required) Is the CIK number of the company you are searching.\n",
    "\n",
    "- **type**: (optional) Allows filtering the type of form. For example, if set to 10-k only the 10-K filings are returned.\n",
    "\n",
    "- **dateb**: (optional) Will only return the filings before a given date. The format is as follows YYYYMMDD\n",
    "\n",
    "- **owner:** (required) Is set to exclude by default and specifies ownership. You may also set it to include and only.\n",
    "\n",
    "- **start:** (optional) Is the starting index of the results. For example, if I have 100 results but want to start at 45 of 100, I would pass 45.\n",
    "\n",
    "- **state:** (optional) The company's state.\n",
    "\n",
    "- **filenum:** (optional) The filing number.\n",
    "\n",
    "- **sic:** (optional) The company's SIC (Standard Industry Classification) identifier\n",
    "- **output:** (optional) Defines returned data structure as either xml (atom) or normal html.\n",
    "\n",
    "- **count:** (optional) The number of results you want to see with your request, the max is 100 and if not set it will default to 40.\n",
    "\n",
    "------------------------------------------------------------------------------\n",
    "Now that we understand all the parameters let's make a request by defining our endpoint, and then a dictionary of our parameters. Where the key of the dictionary is the parameter name, and the value is the value we want to set for that parameter. Once we've defined these two components we can make our request and parse the response using BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request Successful\n",
      "https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=1265107&type=10-k&dateb=20200101&owner=exclude&start=&output=&count=100\n"
     ]
    }
   ],
   "source": [
    "# base URL for the SEC EDGAR browser\n",
    "endpoint = r\"https://www.sec.gov/cgi-bin/browse-edgar\"\n",
    "\n",
    "# define our parameters dictionary\n",
    "param_dict = {'action':'getcompany',\n",
    "              'CIK':'1265107',\n",
    "              'type':'10-k',\n",
    "              'dateb':'20200101',\n",
    "              'owner':'exclude',\n",
    "              'start':'',\n",
    "              'output':'',\n",
    "              'count':'100'}\n",
    "\n",
    "# request the url, and then parse the response.\n",
    "response = requests.get(url = endpoint, params = param_dict)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Let the user know it was successful.\n",
    "print('Request Successful')\n",
    "print(response.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side Note: Doing a Company Name Search\n",
    "\n",
    "In the search defined up above, I assumed we wanted to search by a CIK number. If this is not the case, we will do a different search, a company search. A company search is a more broad search but is simpler because it requires fewer parameters. The only new parameter we have to pass through is the company parameter which has the company name as it's value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request Successful\n",
      "https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&owner=Bill+Gates&company=microsoft\n"
     ]
    }
   ],
   "source": [
    "# base URL for the SEC EDGAR browser\n",
    "endpoint = r\"https://www.sec.gov/cgi-bin/browse-edgar\"\n",
    "\n",
    "# define our parameters dictionary\n",
    "param_dict = {'action':'getcompany',\n",
    "              'owner':'Bill Gates',\n",
    "              'company':'microsoft'}\n",
    "\n",
    "# request the url, and then parse the response.\n",
    "response = requests.get(url = endpoint, params = param_dict)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Let the user know it was successful.\n",
    "print('Request Successful')\n",
    "print(response.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section Two: Parse the Response for the Document Details\n",
    "\n",
    "Once we have our response code, we need to parse it. Our first goal is to find the table object that has a class attribute of tableFile2 as this table contains the data related to the documents and the links. Once we grabbed the table, we will iterate through each row in the table parsing the columns. Unfortunately, things can get a little tricky as some documents can contain interactive data and others don't this means that if parse them for links it will return an error because no link exists.\n",
    "\n",
    "To fix this, I wrap each link in an if statement that will only parse the href if one exists. The main links I'm looking for are the links to the document itself, the interactive data, and a link to filing folder containing that document. Keep in mind when I say filing folder what I mean is another EDGAR query but with an additional parameter called filenum which contains the filing number. Once, we parsed the necessary info, we store it in a dictionary and then store that dictionary in the master_list, this way we have a list of all the file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Filing Type: 10-K\n",
      "Filing Date: 2019-04-01\n",
      "Filing Number: 333-11002519722003\n",
      "Document Link: https://www.sec.gov/Archives/edgar/data/1265107/000126510719000004/0001265107-19-000004-index.htm\n",
      "Filing Number Link: https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&filenum=333-110025&owner=exclude&count=100\n",
      "Interactive Data Link: https://www.sec.gov/cgi-bin/viewer?action=view&cik=1265107&accession_number=0001265107-19-000004&xbrl_type=v\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Filing Type: 10-K\n",
      "Filing Date: 2018-03-07\n",
      "Filing Number: 333-11002518671437\n",
      "Document Link: https://www.sec.gov/Archives/edgar/data/1265107/000126510718000013/0001265107-18-000013-index.htm\n",
      "Filing Number Link: https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&filenum=333-110025&owner=exclude&count=100\n",
      "Interactive Data Link: https://www.sec.gov/cgi-bin/viewer?action=view&cik=1265107&accession_number=0001265107-18-000013&xbrl_type=v\n"
     ]
    }
   ],
   "source": [
    "# find the document table with our data\n",
    "doc_table = soup.find_all('table', class_='tableFile2')\n",
    "\n",
    "# define a base url that will be used for link building.\n",
    "base_url_sec = r\"https://www.sec.gov\"\n",
    "\n",
    "master_list = []\n",
    "\n",
    "# loop through each row in the table.\n",
    "for row in doc_table[0].find_all('tr')[0:3]:\n",
    "    \n",
    "    # find all the columns\n",
    "    cols = row.find_all('td')\n",
    "    \n",
    "    # if there are no columns move on to the next row.\n",
    "    if len(cols) != 0:        \n",
    "        \n",
    "        # grab the text\n",
    "        filing_type = cols[0].text.strip()                 \n",
    "        filing_date = cols[3].text.strip()\n",
    "        filing_numb = cols[4].text.strip()\n",
    "        \n",
    "        # find the links\n",
    "        filing_doc_href = cols[1].find('a', {'href':True, 'id':'documentsbutton'})       \n",
    "        filing_int_href = cols[1].find('a', {'href':True, 'id':'interactiveDataBtn'})\n",
    "        filing_num_href = cols[4].find('a')\n",
    "        \n",
    "        # grab the the first href\n",
    "        if filing_doc_href != None:\n",
    "            filing_doc_link = base_url_sec + filing_doc_href['href'] \n",
    "        else:\n",
    "            filing_doc_link = 'no link'\n",
    "        \n",
    "        # grab the second href\n",
    "        if filing_int_href != None:\n",
    "            filing_int_link = base_url_sec + filing_int_href['href'] \n",
    "        else:\n",
    "            filing_int_link = 'no link'\n",
    "            \n",
    "        # grab the third href\n",
    "        if filing_num_href != None:\n",
    "            filing_num_link = base_url_sec + filing_num_href['href'] \n",
    "        else:\n",
    "            filing_num_link = 'no link'\n",
    "        \n",
    "        # create and store data in the dictionary\n",
    "        file_dict = {}\n",
    "        file_dict['file_type'] = filing_type\n",
    "        file_dict['file_number'] = filing_numb\n",
    "        file_dict['file_date'] = filing_date\n",
    "        file_dict['links'] = {}\n",
    "        file_dict['links']['documents'] = filing_doc_link\n",
    "        file_dict['links']['interactive_data'] = filing_int_link\n",
    "        file_dict['links']['filing_number'] = filing_num_link\n",
    "    \n",
    "        # let the user know it's working\n",
    "        print('-'*100)        \n",
    "        print(\"Filing Type: \" + filing_type)\n",
    "        print(\"Filing Date: \" + filing_date)\n",
    "        print(\"Filing Number: \" + filing_numb)\n",
    "        print(\"Document Link: \" + filing_doc_link)\n",
    "        print(\"Filing Number Link: \" + filing_num_link)\n",
    "        print(\"Interactive Data Link: \" + filing_int_link)\n",
    "        \n",
    "        # append dictionary to master list\n",
    "        master_list.append(file_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section Three: Parsing the Master List\n",
    "\n",
    "We the master_list now populated we can iterate through the dictionary in the list and grab the values we want from each dictionary by passing the keys corresponding to that value. In the example below, I want all the links from a given dictionary, so I parse the links dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "https://www.sec.gov/Archives/edgar/data/1265107/000126510719000004/0001265107-19-000004-index.htm\n",
      "https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&filenum=333-110025&owner=exclude&count=100\n",
      "https://www.sec.gov/cgi-bin/viewer?action=view&cik=1265107&accession_number=0001265107-19-000004&xbrl_type=v\n",
      "----------------------------------------------------------------------------------------------------\n",
      "https://www.sec.gov/Archives/edgar/data/1265107/000126510718000013/0001265107-18-000013-index.htm\n",
      "https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&filenum=333-110025&owner=exclude&count=100\n",
      "https://www.sec.gov/cgi-bin/viewer?action=view&cik=1265107&accession_number=0001265107-18-000013&xbrl_type=v\n"
     ]
    }
   ],
   "source": [
    "# loop through to get the links from the dictionary\n",
    "\n",
    "for report in master_list[0:]:\n",
    "    \n",
    "    print('-'*100)\n",
    "    print(report['links']['documents'])\n",
    "    print(report['links']['filing_number'])\n",
    "    print(report['links']['interactive_data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section Four: Parsing the XML version\n",
    "\n",
    "We saw up above that if we set the output parameter to atom, that we get back an XML version of the same data, so let's explore how to request and parse the XML output. When we are defining the output parameter, we are accessing the RSS Feed that is linked with EDGAR. While the above example does work relatively easily, it probably makes more sense to use the RSS Feed as the data returned to us is more structured and therefore easier to parse.\n",
    "\n",
    "The request will be identical except for the fact that we will change the output parameter to atom and change the parser to lxml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request Successful\n",
      "https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=1265107&type=10-k&dateb=20190101&owner=exclude&start=&output=atom&count=100\n"
     ]
    }
   ],
   "source": [
    "# base URL for the SEC EDGAR browser\n",
    "endpoint = r\"https://www.sec.gov/cgi-bin/browse-edgar\"\n",
    "\n",
    "# define our parameters dictionary\n",
    "param_dict = {'action':'getcompany',\n",
    "              'CIK':'1265107',\n",
    "              'type':'10-k',\n",
    "              'dateb':'20190101',\n",
    "              'owner':'exclude',\n",
    "              'start':'',\n",
    "              'output':'atom',\n",
    "              'count':'100'}\n",
    "\n",
    "# request the url, and then parse the response.\n",
    "response = requests.get(url = endpoint, params = param_dict)\n",
    "soup = BeautifulSoup(response.content, 'lxml')\n",
    "\n",
    "# Let the user know it was successful.\n",
    "print('Request Successful')\n",
    "print(response.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the content, we need to a search for all the entry tags as these tags contain the info related to the filings. Each entry tag has the following structure:\n",
    "```\n",
    "<entry>,\n",
    "        <category label=\\\"form type\\\" scheme=\\\"https://www.sec.gov/\\\" term=\\\"10-K\\\"> </category>\n",
    "        <content type =\\\"text/xml\\\">\n",
    "            <accession-nunber></accession-nunber>\n",
    "            <act></act>\n",
    "            <file-number></file-number>\n",
    "            <file-number-href></file-number-href>\n",
    "            <filing-date></filing-date>\n",
    "            <filing-href></filing-href>\n",
    "            <filing-type></filing-type>\n",
    "            <film-number></film-number>\n",
    "            <form-name></form-name>\n",
    "            <size></size>\n",
    "            <xbrl_href></xbrl_href>,\n",
    "        </content>\n",
    "        <id></id>\n",
    "        <link href=\\\"\\\" rel=\\\"alternate\\\" type=\\\"text/html\\\"/>\n",
    "        <summary type=\\\"html\\\"></summary>\n",
    "        <title></title>\n",
    "        <updated></updated>\n",
    "    </entry>\n",
    "```\n",
    "Please keep in mind that I have removed the actual info for readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Annual report [Section 13 and 15(d), not S-K Item 405]\n",
      "333-110025\n",
      "https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&filenum=333-110025&owner=exclude&count=100\n",
      "https://www.sec.gov/Archives/edgar/data/1265107/000126510718000013/0001265107-18-000013-index.htm\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Annual report [Section 13 and 15(d), not S-K Item 405]\n",
      "333-110025\n",
      "https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&filenum=333-110025&owner=exclude&count=100\n",
      "https://www.sec.gov/Archives/edgar/data/1265107/000126510717000007/0001265107-17-000007-index.htm\n"
     ]
    }
   ],
   "source": [
    "entries = soup.find_all('entry')\n",
    "# print(entries)\n",
    "\n",
    "# initalize our list for storage\n",
    "master_list_xml = []\n",
    "\n",
    "# loop through each found entry, remember this is only the first two\n",
    "for entry in entries[0:2]:\n",
    "    # grab the accession number so we can create a key value\n",
    "    accession_num = entry.find('accession-number').text\n",
    "    \n",
    "    # create a new dictionary\n",
    "    entry_dict = {}\n",
    "    entry_dict[accession_num] = {}\n",
    "    \n",
    "    # store the category info\n",
    "    category_info = entry.find('category')    \n",
    "    entry_dict[accession_num]['category'] = {}\n",
    "    entry_dict[accession_num]['category']['label'] = category_info['label']\n",
    "    entry_dict[accession_num]['category']['scheme'] = category_info['scheme']\n",
    "    entry_dict[accession_num]['category']['term'] =  category_info['term']\n",
    "\n",
    "    # store the file info\n",
    "    entry_dict[accession_num]['file_info'] = {}\n",
    "    entry_dict[accession_num]['file_info']['act'] = entry.find('act').text\n",
    "    entry_dict[accession_num]['file_info']['file_number'] = entry.find('file-number').text\n",
    "    entry_dict[accession_num]['file_info']['file_number_href'] = entry.find('file-number-href').text\n",
    "    entry_dict[accession_num]['file_info']['filing_date'] =  entry.find('filing-date').text\n",
    "    entry_dict[accession_num]['file_info']['filing_href'] = entry.find('filing-href').text\n",
    "    entry_dict[accession_num]['file_info']['filing_type'] =  entry.find('filing-type').text\n",
    "    entry_dict[accession_num]['file_info']['form_number'] =  entry.find('film-number').text\n",
    "    entry_dict[accession_num]['file_info']['form_name'] =  entry.find('form-name').text\n",
    "    entry_dict[accession_num]['file_info']['file_size'] =  entry.find('size').text\n",
    "    \n",
    "    # store extra info\n",
    "    entry_dict[accession_num]['request_info'] = {}\n",
    "    entry_dict[accession_num]['request_info']['link'] =  entry.find('link')['href']\n",
    "    entry_dict[accession_num]['request_info']['title'] =  entry.find('title').text\n",
    "    entry_dict[accession_num]['request_info']['last_updated'] =  entry.find('updated').text\n",
    "    \n",
    "    # store in the master list\n",
    "    master_list_xml.append(entry_dict)\n",
    "    \n",
    "    print('-'*100)\n",
    "    print(entry.find('form-name').text)\n",
    "    print(entry.find('file-number').text)\n",
    "    print(entry.find('file-number-href').text)\n",
    "    print(entry.find('link')['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------\n",
    "Now that we have all the entries stored in our dictionary let's grab the first item and see what the output looks like for the category section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'form type', 'scheme': 'https://www.sec.gov/', 'term': '10-K'}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(master_list_xml[0]['0001265107-18-000013']['category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the Next Page\n",
    "\n",
    "In the example above our results were limited because we did such a narrow search, but it's not uncommon for more broad searches to return over 100 different entries. In these situations, we can leverage the XML output to find the link that takes us to the additional results. This process is easy; we merely find the link tag that has a rel attribute set to next. To demonstrate this, I've added a new URL that will return over 100 results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=0001265107&type=&datea=&dateb=20190101&owner=exclude&count=40&output=atom&start=40\n",
      "----------------------------------------------------------------------------------------------------\n",
      "https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=0001265107&type=&datea=&dateb=20190101&owner=exclude&count=40&output=atom&start=80\n",
      "----------------------------------------------------------------------------------------------------\n",
      "https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=0001265107&type=&datea=&dateb=20190101&owner=exclude&count=40&output=atom&start=120\n",
      "----------------------------------------------------------------------------------------------------\n",
      "https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=0001265107&type=&datea=&dateb=20190101&owner=exclude&count=40&output=atom&start=160\n"
     ]
    }
   ],
   "source": [
    "# base URL for the SEC EDGAR browser\n",
    "endpoint = r\"https://www.sec.gov/cgi-bin/browse-edgar\"\n",
    "\n",
    "# define our parameters dictionary\n",
    "param_dict = {'action':'getcompany',\n",
    "              'CIK':'1265107',\n",
    "              'dateb':'20190101',\n",
    "              'owner':'exclude',\n",
    "              'start':'',\n",
    "              'output':'atom',\n",
    "              'count':'50'}\n",
    "\n",
    "# request the url, and then parse the response.\n",
    "response = requests.get(url = endpoint, params = param_dict)\n",
    "soup = BeautifulSoup(response.content, 'lxml')\n",
    "\n",
    "# find the link that will take us to the next page\n",
    "links = soup.find_all('link',{'rel':'next'})\n",
    "\n",
    "# while there is still a next page\n",
    "while soup.find_all('link',{'rel':'next'}) != []:\n",
    "\n",
    "    # grab the link\n",
    "    next_page_link = links[0]['href']  \n",
    "    \n",
    "    print('-'*100)\n",
    "    print(next_page_link)\n",
    "    \n",
    "    # request the next page\n",
    "    response = requests.get(url = next_page_link)\n",
    "    soup = BeautifulSoup(response.content, 'lxml')\n",
    "    \n",
    "    # see if there is a next link\n",
    "    links = soup.find_all('link',{'rel':'next'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
