{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are going to extract information from daily index:\n",
    "https://www.sec.gov/Archives/edgar/daily-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.sec.gov/Archives/edgar/data/886982/000156459019011378/0001564590-19-011378-index-headers.html'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's first make a function that will make the process of building a url easy.\n",
    "def make_url(base_url , comp):\n",
    "    \n",
    "    url = base_url\n",
    "    \n",
    "    # add each component to the base url\n",
    "    for r in comp:\n",
    "        url = '{}/{}'.format(url, r)\n",
    "        \n",
    "    return url\n",
    "\n",
    "# EXAMPLE\n",
    "base_url = r\"https://www.sec.gov/Archives/edgar/data\"\n",
    "components = ['886982','000156459019011378', '0001564590-19-011378-index-headers.html']\n",
    "make_url(base_url, components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all documents of a year\n",
    "\n",
    "Here we are going to get the links of all documents of all companies in a specific year\n",
    "divided by quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'directory': {'item': [{'last-modified': '03/31/2020 10:08:05 PM',\n",
       "    'name': 'QTR1',\n",
       "    'type': 'dir',\n",
       "    'href': 'QTR1/',\n",
       "    'size': '20 KB'},\n",
       "   {'last-modified': '06/26/2020 10:07:19 PM',\n",
       "    'name': 'QTR2',\n",
       "    'type': 'dir',\n",
       "    'href': 'QTR2/',\n",
       "    'size': '20 KB'},\n",
       "   {'last-modified': '07/01/2020 12:20:09 AM',\n",
       "    'name': 'QTR3',\n",
       "    'type': 'dir',\n",
       "    'href': 'QTR3/',\n",
       "    'size': '4 KB'}],\n",
       "  'name': 'daily-index/2020/',\n",
       "  'parent-dir': '../'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# base url for the daily index files.\n",
    "base_url = r\"https://www.sec.gov/Archives/edgar/daily-index\"\n",
    "\n",
    "# create the URL of daily index for 2020\n",
    "year_url = make_url(base_url, ['2020', 'index.json'])\n",
    "\n",
    "# request year_url\n",
    "content = requests.get(year_url)\n",
    "\n",
    "# convert data from json to dict\n",
    "decoded_content = content.json()\n",
    "decoded_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop trhough the dict\n",
    "for item in decoded_content['directory']['item']:\n",
    "    \n",
    "    # get the name of the folder\n",
    "    print(\"-\" * 100)\n",
    "    print(\"pulling url for quarter {}\".format(item[\"name\"]))\n",
    "    \n",
    "    # create the qtr url\n",
    "    qtr_url = make_url(base_url, ['2020',item[\"name\"], 'index.json'])\n",
    "    print(qtr_url)\n",
    "    \n",
    "    # request qtr_url\n",
    "    file_content = requests.get(qtr_url)\n",
    "\n",
    "    # convert data from json to dict\n",
    "    decoded_content = file_content.json()\n",
    "    \n",
    "    print(\"-\" * 100)\n",
    "    print(\"Pulling files\")\n",
    "    \n",
    "    for file in decoded_content['directory']['item']:\n",
    "        file_url = make_url(base_url, ['2020', item[\"name\"], file['name']])\n",
    "        print(file_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Parsing the master IDX file\n",
    "\n",
    "Out of all the files to parse, I find the Master.idx file the easiest because it is possible to separate each field by a delimiter. Whereas the other files do not offer such a delimiter or they lack the additional detail that is provided by master file. With that being said, if I had to choose a second file to parse, it would probably be the sitemap file because of the provided structure in it.\n",
    "\n",
    "The first thing is to load the information to a text file so that way you don't have to make a second request and not burden the server. After we create a new text file with the content, we can reload it into by opening the text file. From here, I usually encourage people to explore the data before they perform any parsing. We will notice right away that getting the info may be a little challenging, but it can be done.\n",
    "\n",
    "The approach that I laid out below worked for most files I encountered, but I cannot guarantee it will work for all of them. As time goes on you, have more detailed data so parsing the dataset will become more comfortable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a url, in this case I'll just take one of the urls up above.\n",
    "file_url = r\"https://www.sec.gov/Archives/edgar/daily-index/2020/QTR2/master.20200401.idx\"\n",
    "\n",
    "# request that new content, this will not be a JSON STRUCTURE!\n",
    "# this will download the file with all data of that day 2020-04-01\n",
    "# and store the content into content var\n",
    "content = requests.get(file_url).content\n",
    "\n",
    "# we can always write the content to a file, so we don't need to request it again.\n",
    "with open('master_20200401.txt', 'wb') as f:\n",
    "     f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1189229',\n",
       "  'PAISLEY CHRISTOPHER B',\n",
       "  '4',\n",
       "  '20200401',\n",
       "  'https://www.sec.gov/Archives/edgar/data/1189229/0001209191-20-022250.txt'],\n",
       " ['1189425',\n",
       "  'POTTER ROBERT L',\n",
       "  '4',\n",
       "  '20200401',\n",
       "  'https://www.sec.gov/Archives/edgar/data/1189425/0001121484-20-000050.txt'],\n",
       " ['1189703',\n",
       "  'TELLEZ CORA M',\n",
       "  '4',\n",
       "  '20200401',\n",
       "  'https://www.sec.gov/Archives/edgar/data/1189703/0001225208-20-005755.txt']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's open it and we will now have a byte stream to play with.\n",
    "with open('master_20200401.txt','rb') as f:\n",
    "     byte_data = f.read()\n",
    "\n",
    "# Now that we loaded the data, we have a byte stream that needs to be decoded and then split by double spaces.\n",
    "data = byte_data.decode(\"utf-8\").split('  ')\n",
    "\n",
    "# removing the headers, so look for the end of the header and grab it's index\n",
    "for index, item in enumerate(data):\n",
    "    if \"ftp://ftp.sec.gov/edgar/\" in item:\n",
    "        start_ind = index\n",
    "\n",
    "# define a new dataset without the headers.\n",
    "data_format = data[start_ind + 1:]\n",
    "\n",
    "master_data = []\n",
    "\n",
    "# loop through the data list\n",
    "for index, item in enumerate(data_format):\n",
    "    # if it's the first index, it won't be even so treat it differently\n",
    "    if index == 0:\n",
    "        clean_item_data = item.replace('\\n','|').split('|')\n",
    "        clean_item_data = clean_item_data[8:]\n",
    "    else:\n",
    "        clean_item_data = item.replace('\\n','|').split('|')\n",
    "    \n",
    "    for index, row in enumerate(clean_item_data):\n",
    "\n",
    "        if \".txt\" in row:\n",
    "            mini_list = clean_item_data[(index - 4): index + 1]\n",
    "            \n",
    "            if len(mini_list) != 0:\n",
    "                mini_list[4] = \"https://www.sec.gov/Archives/\" + mini_list[4]\n",
    "                master_data.append(mini_list)\n",
    "master_data[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating our Document Dictionary\n",
    "\n",
    "An extra step we can take is converting our master list of data into a list of dictionaries, where each dictionary represents a single filing document. This way we can quickly iterate over the master list to grab the data we need. This structure will help us down the road when we need to access only some aspects of information. I encourage individuals to put the time up in front to ensure a quick and easy process for the bulk of the parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through each document in the master list.\n",
    "for index, document in enumerate(master_data):\n",
    "    \n",
    "    # create a dictionary for each document in the master list\n",
    "    document_dict = {}\n",
    "    document_dict['cik_number'] = document[0]\n",
    "    document_dict['company_name'] = document[1]\n",
    "    document_dict['form_id'] = document[2]\n",
    "    document_dict['date'] = document[3]\n",
    "    document_dict['file_url'] = document[4]\n",
    "    \n",
    "    master_data[index] = document_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'cik_number': '1189229',\n",
       "  'company_name': 'PAISLEY CHRISTOPHER B',\n",
       "  'form_id': '4',\n",
       "  'date': '20200401',\n",
       "  'file_url': 'https://www.sec.gov/Archives/edgar/data/1189229/0001209191-20-022250.txt'},\n",
       " {'cik_number': '1189425',\n",
       "  'company_name': 'POTTER ROBERT L',\n",
       "  'form_id': '4',\n",
       "  'date': '20200401',\n",
       "  'file_url': 'https://www.sec.gov/Archives/edgar/data/1189425/0001121484-20-000050.txt'},\n",
       " {'cik_number': '1189703',\n",
       "  'company_name': 'TELLEZ CORA M',\n",
       "  'form_id': '4',\n",
       "  'date': '20200401',\n",
       "  'file_url': 'https://www.sec.gov/Archives/edgar/data/1189703/0001225208-20-005755.txt'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_data[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering by File Type\n",
    "\n",
    "Naturally, we might not need all the files that we have scraped, so let's explore how to filter the data. If we want to grab all the 10-K filings from the dataset, we loop through our master_data list and only print the ones where the form_id has a value of 10-K. In the example below, I only loop through the first 100 dictionaries for readability.\n",
    "\n",
    "Finally, I do some extra transformation on the final document URL to set the stage for the next tutorial. The beautiful thing about these document URLs is that with a few other transformations we can now get to that particular company filing archive.\n",
    "\n",
    "If you would like more info on the different financial forms that we have access to, I encourage you to visit https://www.sec.gov/forms for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "CREATIVE LEARNING Corp\n",
      "https://www.sec.gov/Archives/edgar/data/1394638/0001731122-20-000338.txt\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Predictive Oncology Inc.\n",
      "https://www.sec.gov/Archives/edgar/data/1446159/0001171843-20-002217.txt\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MARIMED INC.\n",
      "https://www.sec.gov/Archives/edgar/data/1522767/0001493152-20-005546.txt\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TILLY'S, INC.\n",
      "https://www.sec.gov/Archives/edgar/data/1524025/0001628280-20-004436.txt\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Vislink Technologies, Inc.\n",
      "https://www.sec.gov/Archives/edgar/data/1565228/0001493152-20-005537.txt\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TurnKey Capital, Inc.\n",
      "https://www.sec.gov/Archives/edgar/data/1567503/0001553350-20-000289.txt\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Village Farms International, Inc.\n",
      "https://www.sec.gov/Archives/edgar/data/1584549/0001193125-20-094235.txt\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GREY CLOAK TECH INC.\n",
      "https://www.sec.gov/Archives/edgar/data/1630176/0001520138-20-000130.txt\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pineapple Express, Inc.\n",
      "https://www.sec.gov/Archives/edgar/data/1654672/0001493152-20-005517.txt\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GENESCO INC\n",
      "https://www.sec.gov/Archives/edgar/data/18498/0000018498-20-000016.txt\n",
      "----------------------------------------------------------------------------------------------------\n",
      "KROGER CO\n",
      "https://www.sec.gov/Archives/edgar/data/56873/0001558370-20-003501.txt\n",
      "----------------------------------------------------------------------------------------------------\n",
      "REX AMERICAN RESOURCES Corp\n",
      "https://www.sec.gov/Archives/edgar/data/744187/0000930413-20-000944.txt\n",
      "----------------------------------------------------------------------------------------------------\n",
      "PVH CORP. /DE/\n",
      "https://www.sec.gov/Archives/edgar/data/78239/0000078239-20-000023.txt\n",
      "----------------------------------------------------------------------------------------------------\n",
      "True Nature Holding, Inc.\n",
      "https://www.sec.gov/Archives/edgar/data/802257/0001185185-20-000411.txt\n",
      "----------------------------------------------------------------------------------------------------\n",
      "BUCKLE INC\n",
      "https://www.sec.gov/Archives/edgar/data/885245/0000885245-20-000011.txt\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GUESS INC\n",
      "https://www.sec.gov/Archives/edgar/data/912463/0000912463-20-000020.txt\n"
     ]
    }
   ],
   "source": [
    "# by being in a dictionary format, it'll be easier to get the items we need.\n",
    "for document_dict in master_data:\n",
    "#     print(document_dict['form_id'] )\n",
    "\n",
    "    # if it's a 10-K document pull the url and the name.\n",
    "    if document_dict['form_id'] == '10-K':\n",
    "        \n",
    "        # get the components\n",
    "        comp_name = document_dict['company_name']\n",
    "        docu_url = document_dict['file_url']\n",
    "        \n",
    "        print('-'*100)\n",
    "        print(comp_name)\n",
    "        print(docu_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
